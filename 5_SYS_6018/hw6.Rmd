---
title: "Homework #6: Clustering" 
author: "**Chloe Fauvel**"
date: "Due: Mon Apr 5 | 10:55am"
output: R6018::homework
---

**SYS 4582/6018 | Spring 2021 | University of Virginia **

*******************************************
```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6018")) # knitr settings
# options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```


# Required R packages and Directories

### {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/SYS6018/data/' # data directory
library(R6018)     # functions for SYS-6018
library(tidyverse) # functions for data manipulation   
library(mclust)    # functions for mixture models
library(mixtools)  # poisregmixEM() function
```


# Problem 1: Customer Segmentation with RFM (Recency, Frequency, and Monetary Value)

RFM analysis is an approach that some businesses use to understand their customers' activities. At any point in time, a company can measure how recently a customer purchased a product (Recency), how many times they purchased a product (Frequency), and how much they have spent (Monetary Value). There are many ad-hoc attempts to segment/cluster customers based on the RFM scores (e.g., here is one based on using the customers' rank of each dimension independently: <https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html>). In this problem you will use the clustering methods we covered in class to segment the customers. 


The data for this problem can be found here: <`r file.path(data.dir, "RFM.csv")`>. Cluster based on the Recency, Frequency, and Monetary value columns.


## a. Implement hierarchical clustering. 

- Describe any pre-processing steps you took (e.g., scaling, distance metric)
- State the linkage method you used with justification. 
- Show the resulting dendrogram
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 100 in the same cluster?     
    
### {.solution}

Because the scales for the variables recency, frequency, and monetary value are different and I am using Euclidean distance as the dissimilarity measure, I chose to apply the scale() function on these predictor variables. I also applied the dist() function to get the distance matrix between all observations.

```{r}
data <- read.csv("RFM.csv", header=TRUE)
data <- select(data,-id)

scaled.data <- scale(data)
dX <- dist(scaled.data,method="euclidean")

set.seed(100) #for reproducible results
hc <- hclust(dX,method="average")
```

The linkage method I chose was average linkage because I like it and makes the most sense to me.

```{r}
plot(hc)

chooseK <- tibble(height=hc$height, K=row_number(-height))

ggplot(chooseK, aes(K,height)) + geom_line() +
  geom_point()
```

There is no obvious elbow in the above graph to help decide the number of clusters, so based on the dendrogram, I chose 2 clusters because of the large height difference.

```{r}
yhat <- cutree(hc, k=2)
yhat[1]
yhat[100]
```

Yes, customers 1 and 100 are in the same cluster.

## b. Implement k-means.  

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 100 in the same cluster?     
    
### {.solution}

I used the scaled data from Part a for the same reasons.

```{r, warning=FALSE}
set.seed(100) #for reproducible results

Kmax = 20

SSE = numeric(Kmax)

for (k in 1:Kmax) {
  km = kmeans(scaled.data,centers=k,nstart=25)
  SSE[k]=km$tot.withinss
}

plot(1:Kmax, SSE, type='o', las=1, xlab="K")
```

Based on the above graph, I chose K=4 because of the steeper slope between points K=3 and K=4 vs K=4 and K=5.

```{r}
km = kmeans(scaled.data, centers=4, nstart=25)
km$cluster[1]
km$cluster[100]
```

No, customers 1 and 100 are not in the same cluster. 

## c. Implement model-based clustering

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Describe the best model. What restrictions are on the shape of the components?
- Using your segmentation, are customers 1 and 100 in the same cluster?     

### {.solution}

I chose to use the original data because the lecture slides notes that model-based clustering can fit data that are different variances/densities and the example (page 36) used the original data.

```{r}
set.seed(100)
models = Mclust(data, verbose=FALSE)
summary(models)
```

It appears the Mclust() found VVV (ellipsoidal distribution, variable volume, variable shape, and variable orientation) to be the best restrictions along with K=7 clusters.

```{r}
models$classification[1]
models$classification[100]
```

No, customers 1 and 100 are not in the same cluster.

## d. Discuss how you would cluster the customers if you had to do this for your job. Do you think one model would do better than the others? 

### {.solution}

I would do model-based clustering because of how robust it is. I also prefer being able to use the original data instead of having to use scaled data. 



# Problem 2: Poisson Mixture Model

The pmf of a Poisson random variable is:
\begin{align*}
f_k(x; \lambda_k) = \frac{\lambda_k^x e^{-\lambda_k}}{x!}
\end{align*}

A two-component Poisson mixture model can be written:
\begin{align*}
f(x; \theta) = \pi \frac{\lambda_1^x e^{-\lambda_1}}{x!} + (1-\pi) \frac{\lambda_2^x e^{-\lambda_2}}{x!}
\end{align*}



## a. What are the parameters of the model? 

### {.solution}

The parameters are: $\pi$, $\lambda_1$, $\lambda_2$


## b. Write down the log-likelihood for $n$ independent observations ($x_1, x_2, \ldots, x_n$). 

### {.solution}

$l(\theta, f)=\sum_{i=1}^n\text{log}[\pi\frac{\lambda_1^{x_i}e^-{\lambda_1}}{x_i!}+(1-\pi)\frac{\lambda_2^{x_i}e^-{\lambda_2}}{x_i!}]$


## c. Suppose we have initial values of the parameters. Write down the equation for updating the *responsibilities*. 

### {.solution}

Responsibility $r_{ik}$ is the posterior probability that event $i$ came from component $k$:

$r_{ik}=\frac{\pi_k\text{P}(x_i|g_i=k,\theta_k)}{\sum_{j=1}^K\pi_j\text{P}(x_i|g_i=j,\theta_j)}$

Therefore:
$r_{i1}=\frac{\pi[\frac{\lambda_1^{x_i}e^{-\lambda_1}}{x_i!}]}{\pi[\frac{\lambda_1^{x_i}e^{-\lambda_1}}{x_i!}]+(1-\pi)[\frac{\lambda_2^{x_i}e^{-\lambda_2}}{x_i!}]}=\frac{\pi[{\lambda_1^{x_i}e^{-\lambda_1}}]}{\pi[{\lambda_1^{x_i}e^{-\lambda_1}}]+(1-\pi)[{\lambda_2^{x_i}e^{-\lambda_2}}]}$

$r_{i2}=\frac{(1-\pi)[\frac{\lambda_2^{x_i}e^{-\lambda_2}}{x_i!}]}{\pi[\frac{\lambda_1^{x_i}e^{-\lambda_1}}{x_i!}]+(1-\pi)[\frac{\lambda_2^{x_i}e^{-\lambda_2}}{x_i!}]}=\frac{(1-\pi)[{\lambda_2^{x_i}e^{-\lambda_2}}]}{\pi[{\lambda_1^{x_i}e^{-\lambda_1}}]+(1-\pi)[{\lambda_2^{x_i}e^{-\lambda_2}}]}$


## d. Suppose we have responsibilities, $r_{ik}$ for all $i=1, 2, \ldots, n$ and $k=1,2$. Write down the equations for updating the parameters. 

### {.solution}

Since $\lambda$ is the mean for a Poisson distribution, I am using the updated parameters estimation equations from slide 27 in the lecture slides for clustering:

$\hat{\pi}=\frac{\sum_{i=1}^nr_{i1}}{n}$

$\hat{\lambda_1}=\hat{\mu_1}=\frac{\sum_{i=1}^nr_{i1}x_i}{\sum_{i=1}^nr_{i1}}$

$\hat{\lambda_2}=\hat{\mu_2}=\frac{\sum_{i=1}^nr_{i2}x_i}{\sum_{i=1}^nr_{i2}}$


## e. Fit a two-component Poisson mixture model, report the estimated parameter values, and show a plot of the estimated mixture pmf for the following data:

```{r, echo=TRUE}
#-- Run this code to generate the data
set.seed(123)             # set seed for reproducibility
n = 200                   # sample size
z = sample(1:2, size=n, replace=TRUE, prob=c(.25, .75)) # sample the latent class
theta = c(8, 16)          # true parameters
y = ifelse(z==1, rpois(n, lambda=theta[1]), rpois(n, lambda=theta[2]))
```


<div style="background-color:lightgrey; display: block; border-color: black; padding:1em">

Note: The function `poisregmixEM()` in the R package `mixtools` is designed to estimate a mixture of *Poisson regression* models. We can still use this function for our problem of density estimation if it is recast as an intercept-only regression. To do so, set the $x$ argument (predictors) to `x = rep(1, length(y))` and `addintercept = FALSE`. 

Look carefully at the output from this model. The `beta` values (regression coefficients) are on the log scale.

</div>


### {.solution}

```{r}
results <- poisregmixEM(y=y, x=rep(1, length(y)), addintercept=FALSE)

pi <- results$lambda[1]
pi

lambda1 <- exp(results$beta[1])
lambda1

lambda2 <- exp(results$beta[2])
lambda2


# Received help from Tom in the Slack channel for this part
pmf <- as.data.frame(results$y)
colnames(pmf) <- "y"

plot_pmf <- function(x) {
  (pi*(((lambda1^x)*exp(-lambda1))/factorial(x))) + 
    ((1-pi)*(((lambda2^x)*exp(-lambda2))/factorial(x)))
}

ggplot(data=pmf, aes(x=y)) + stat_function(fun=plot_pmf) +
  ylab("density") +
  ggtitle("Estimated Poisson Mixture PMF")

```



## f. **2 pts Extra Credit**: Write a function that estimates this two-component Poisson mixture model using the EM approach. Show that it gives the same result as part *e*. 
- Note: you are not permitted to copy code.  Write everything from scratch and use comments to indicate how the code works (e.g., the E-step, M-step, initialization strategy, and convergence should be clear). 
- Cite any resources you consulted to help with the coding. 

### {.solution}

Add Solution Here


